# CODE FUNCTION: Implementation of the Neural Network Algorithm - Artifical Intelligence
# STUDY AREA: Student's Grade Prediction. Artificial Intelligence Applied to Educational
# ARTIFICIAL INTELLIGENCE AREA: Deep Learning (a very, very, very simple network)
# LANGUAGE: Python
# OBJECT: Pragmatic code segment of Artificial Intelligence. A research approach of the IA 
# AUTHOR: Héctor Alonso-Misol Gerlache
# DATE: Dec 2020

# Libraries
import keras
from keras.models import Sequential
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Dense, Dropout, Activation, Embedding, GlobalAveragePooling1D, Flatten
import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler

import matplotlib.pyplot as plt

# Print function: accuracy
def plot_acc(history, title):
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title(title)
    plt.xlim=(0, 30)
    plt.ylim=(0, 100)
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper left')
    plt.show()

# Print function: loss
def plot_loss(history, title):
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title(title)
    plt.xlim=(0, 30)
    plt.ylim=(0, 100)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper right')
    plt.show()

# All the features we need
list_features = []
for col in df_exam_calification.columns:
    list_features.append(col)
# We don´t need this feature
list_features.remove('FINAL_EX_CALIFICATION')

# Train and Test data
X = pd.DataFrame(df_exam_calification[list_features].copy())
y = df_exam_calification['FINAL_EX_CALIFICATION'].copy()
    
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1976)
print (X_train.shape[0] == y_train.shape[0])

# We ensure the data are normalized
X_train = X_train / 255.0
X_test = X_test / 255.0

# Data shape
print('Datos de Entrenamiento: X=%s, y=%s' % (X_train.shape, y_train.shape))
print('Datos de Test: X=%s, y=%s' % (X_test.shape, y_test.shape))

print ('Tenemos {} datos de entrenamiento y {} datos para test'.format(len(X_train), len(X_test)))

# Our min and max data 
print ('La variable y_train tiene un mínimo en: {0} y un máximo en: {1}'.format(y_train.min(), y_train.max()))
print ('La variable y_test tiene un mínimo en: {0} y un máximo en: {1}'.format(y_test.min(), y_test.max()))

# Hiperparameters of our neural netkork: batch_size, epochs and all the classification posibilities
batch_size = 20
epochs = 300
num_classes = 4 # Las cuatro posibles clases: suspenso, aprobado, notable, sobresaliente

# What number of data are we going to train?? What shape?
print (X_train.shape[0], 'datos de entrenamiento')
print (X_test.shape[0], 'datos de test')
X_train.shape

# Minimum resahepe: Data to float32
X_train = X_train.astype('float32')

# Here we go with the correct train and test data
y_train_C = keras.utils.to_categorical(y_train, num_classes)
y_test_C = keras.utils.to_categorical(y_test, num_classes)
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
X_test = X_test.astype('float32')

# Our model: Sequential, full-connected
model = Sequential()
# First hidden-layer, Relu
model.add(Dense(18, activation='relu', input_dim=9))
# Adding dropout is good to evite overfitting
model.add(Dropout(0.2))
# Second hidden-layer, Relu
model.add(Dense(9, activation='relu'))
model.add(Dropout(0.2))
# The last layer, with our 4 possibilites. We should use softmax
model.add(Dense(num_classes, activation='softmax'))

# Model summary: only 391 parameters. 
model.summary()

# Compilation of our simple model
model.compile (optimizer = 'Nadam',
               loss = 'mean_squared_error',
               metrics = ['accuracy'])

# We can monitorize our training
history = model.fit (X_train, y_train_C, batch_size=batch_size, epochs=epochs, verbose=1,
                     validation_data=(X_test, y_test_C))

# And now, we can see the result printing with te loss and accuracy functions
print ("Funciones de accuracy y pérdida")
plot_acc (history, "Model Accuracy")
plot_loss(history, "Model Loss")
